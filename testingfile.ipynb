{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alert: Dropping 199 duplicate rows can sometimes cause column data types to change to object. Double-check!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robin\\AppData\\Local\\Temp\\ipykernel_19504\\3798150608.py:4: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  report = pandas_dq.dq_report(data, target=None, csv_engine=\"pandas\", verbose=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<pandas.io.formats.style.Styler at 0x216d58e5210>",
      "text/html": "<style type=\"text/css\">\n#T_ee81e_row0_col0, #T_ee81e_row0_col2, #T_ee81e_row0_col3, #T_ee81e_row0_col4, #T_ee81e_row0_col5, #T_ee81e_row1_col0, #T_ee81e_row1_col2, #T_ee81e_row1_col3, #T_ee81e_row1_col4, #T_ee81e_row1_col5, #T_ee81e_row2_col0, #T_ee81e_row2_col2, #T_ee81e_row2_col3, #T_ee81e_row2_col4, #T_ee81e_row2_col5, #T_ee81e_row3_col0, #T_ee81e_row3_col2, #T_ee81e_row3_col3, #T_ee81e_row3_col4, #T_ee81e_row3_col5, #T_ee81e_row4_col0, #T_ee81e_row4_col2, #T_ee81e_row4_col3, #T_ee81e_row4_col4, #T_ee81e_row4_col5, #T_ee81e_row5_col0, #T_ee81e_row5_col2, #T_ee81e_row5_col3, #T_ee81e_row5_col4, #T_ee81e_row5_col5, #T_ee81e_row6_col0, #T_ee81e_row6_col2, #T_ee81e_row6_col3, #T_ee81e_row6_col4, #T_ee81e_row6_col5, #T_ee81e_row7_col0, #T_ee81e_row7_col2, #T_ee81e_row7_col3, #T_ee81e_row7_col4, #T_ee81e_row7_col5, #T_ee81e_row8_col0, #T_ee81e_row8_col2, #T_ee81e_row8_col3, #T_ee81e_row8_col4, #T_ee81e_row8_col5, #T_ee81e_row9_col0, #T_ee81e_row9_col2, #T_ee81e_row9_col3, #T_ee81e_row9_col4, #T_ee81e_row9_col5, #T_ee81e_row10_col0, #T_ee81e_row10_col2, #T_ee81e_row10_col3, #T_ee81e_row10_col4, #T_ee81e_row10_col5, #T_ee81e_row11_col0, #T_ee81e_row11_col2, #T_ee81e_row11_col3, #T_ee81e_row11_col4, #T_ee81e_row11_col5, #T_ee81e_row12_col0, #T_ee81e_row12_col2, #T_ee81e_row12_col3, #T_ee81e_row12_col4, #T_ee81e_row12_col5 {\n  font-family: Segoe UI;\n}\n#T_ee81e_row0_col1, #T_ee81e_row1_col1, #T_ee81e_row2_col1, #T_ee81e_row3_col1, #T_ee81e_row4_col1, #T_ee81e_row5_col1, #T_ee81e_row6_col1, #T_ee81e_row7_col1, #T_ee81e_row8_col1, #T_ee81e_row9_col1, #T_ee81e_row10_col1, #T_ee81e_row11_col1, #T_ee81e_row12_col1 {\n  background-color: #fff5f0;\n  color: #000000;\n  font-family: Segoe UI;\n}\n</style>\n<table id=\"T_ee81e\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_ee81e_level0_col0\" class=\"col_heading level0 col0\" >Data Type</th>\n      <th id=\"T_ee81e_level0_col1\" class=\"col_heading level0 col1\" >Missing Values%</th>\n      <th id=\"T_ee81e_level0_col2\" class=\"col_heading level0 col2\" >Unique Values%</th>\n      <th id=\"T_ee81e_level0_col3\" class=\"col_heading level0 col3\" >Minimum Value</th>\n      <th id=\"T_ee81e_level0_col4\" class=\"col_heading level0 col4\" >Maximum Value</th>\n      <th id=\"T_ee81e_level0_col5\" class=\"col_heading level0 col5\" >DQ Issue</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_ee81e_level0_row0\" class=\"row_heading level0 row0\" >Date</th>\n      <td id=\"T_ee81e_row0_col0\" class=\"data row0 col0\" >object</td>\n      <td id=\"T_ee81e_row0_col1\" class=\"data row0 col1\" >0.000000</td>\n      <td id=\"T_ee81e_row0_col2\" class=\"data row0 col2\" >16</td>\n      <td id=\"T_ee81e_row0_col3\" class=\"data row0 col3\" >2015-01-04</td>\n      <td id=\"T_ee81e_row0_col4\" class=\"data row0 col4\" >2017-08-06</td>\n      <td id=\"T_ee81e_row0_col5\" class=\"data row0 col5\" >136 rare categories: Too many to list. Group them into a single category or drop the categories., high cardinality with 136 unique values: Use hash encoding or embedding to reduce dimension.</td>\n    </tr>\n    <tr>\n      <th id=\"T_ee81e_level0_row1\" class=\"row_heading level0 row1\" >AveragePrice</th>\n      <td id=\"T_ee81e_row1_col0\" class=\"data row1 col0\" >float64</td>\n      <td id=\"T_ee81e_row1_col1\" class=\"data row1 col1\" >0.000000</td>\n      <td id=\"T_ee81e_row1_col2\" class=\"data row1 col2\" >NA</td>\n      <td id=\"T_ee81e_row1_col3\" class=\"data row1 col3\" >0.460000</td>\n      <td id=\"T_ee81e_row1_col4\" class=\"data row1 col4\" >2.750000</td>\n      <td id=\"T_ee81e_row1_col5\" class=\"data row1 col5\" >has 8 outliers greater than upper bound (2.4849999999999994) or lower than lower bound(0.2050000000000003). Cap them or remove them.</td>\n    </tr>\n    <tr>\n      <th id=\"T_ee81e_level0_row2\" class=\"row_heading level0 row2\" >Total Volume</th>\n      <td id=\"T_ee81e_row2_col0\" class=\"data row2 col0\" >float64</td>\n      <td id=\"T_ee81e_row2_col1\" class=\"data row2 col1\" >0.000000</td>\n      <td id=\"T_ee81e_row2_col2\" class=\"data row2 col2\" >NA</td>\n      <td id=\"T_ee81e_row2_col3\" class=\"data row2 col3\" >385.550000</td>\n      <td id=\"T_ee81e_row2_col4\" class=\"data row2 col4\" >37130688.910000</td>\n      <td id=\"T_ee81e_row2_col5\" class=\"data row2 col5\" >has 98 outliers greater than upper bound (1262240.94) or lower than lower bound(-741600.02). Cap them or remove them.</td>\n    </tr>\n    <tr>\n      <th id=\"T_ee81e_level0_row3\" class=\"row_heading level0 row3\" >4046</th>\n      <td id=\"T_ee81e_row3_col0\" class=\"data row3 col0\" >float64</td>\n      <td id=\"T_ee81e_row3_col1\" class=\"data row3 col1\" >0.000000</td>\n      <td id=\"T_ee81e_row3_col2\" class=\"data row3 col2\" >NA</td>\n      <td id=\"T_ee81e_row3_col3\" class=\"data row3 col3\" >0.000000</td>\n      <td id=\"T_ee81e_row3_col4\" class=\"data row3 col4\" >14699604.930000</td>\n      <td id=\"T_ee81e_row3_col5\" class=\"data row3 col5\" >has 109 outliers greater than upper bound (292689.375) or lower than lower bound(-174153.145). Cap them or remove them., has a high correlation with ['Total Volume']. Consider dropping one of them.</td>\n    </tr>\n    <tr>\n      <th id=\"T_ee81e_level0_row4\" class=\"row_heading level0 row4\" >4225</th>\n      <td id=\"T_ee81e_row4_col0\" class=\"data row4 col0\" >float64</td>\n      <td id=\"T_ee81e_row4_col1\" class=\"data row4 col1\" >0.000000</td>\n      <td id=\"T_ee81e_row4_col2\" class=\"data row4 col2\" >NA</td>\n      <td id=\"T_ee81e_row4_col3\" class=\"data row4 col3\" >0.000000</td>\n      <td id=\"T_ee81e_row4_col4\" class=\"data row4 col4\" >13926692.620000</td>\n      <td id=\"T_ee81e_row4_col5\" class=\"data row4 col5\" >has 125 outliers greater than upper bound (417847.51999999996) or lower than lower bound(-245488.23999999996). Cap them or remove them., has a high correlation with ['Total Volume', '4046']. Consider dropping one of them.</td>\n    </tr>\n    <tr>\n      <th id=\"T_ee81e_level0_row5\" class=\"row_heading level0 row5\" >4770</th>\n      <td id=\"T_ee81e_row5_col0\" class=\"data row5 col0\" >float64</td>\n      <td id=\"T_ee81e_row5_col1\" class=\"data row5 col1\" >0.000000</td>\n      <td id=\"T_ee81e_row5_col2\" class=\"data row5 col2\" >NA</td>\n      <td id=\"T_ee81e_row5_col3\" class=\"data row5 col3\" >0.000000</td>\n      <td id=\"T_ee81e_row5_col4\" class=\"data row5 col4\" >1326422.560000</td>\n      <td id=\"T_ee81e_row5_col5\" class=\"data row5 col5\" >has 107 outliers greater than upper bound (23569.274999999998) or lower than lower bound(-14141.564999999999). Cap them or remove them., has a high correlation with ['Total Volume', '4046', '4225']. Consider dropping one of them.</td>\n    </tr>\n    <tr>\n      <th id=\"T_ee81e_level0_row6\" class=\"row_heading level0 row6\" >Total Bags</th>\n      <td id=\"T_ee81e_row6_col0\" class=\"data row6 col0\" >float64</td>\n      <td id=\"T_ee81e_row6_col1\" class=\"data row6 col1\" >0.000000</td>\n      <td id=\"T_ee81e_row6_col2\" class=\"data row6 col2\" >NA</td>\n      <td id=\"T_ee81e_row6_col3\" class=\"data row6 col3\" >0.000000</td>\n      <td id=\"T_ee81e_row6_col4\" class=\"data row6 col4\" >10863029.040000</td>\n      <td id=\"T_ee81e_row6_col5\" class=\"data row6 col5\" >has 119 outliers greater than upper bound (293542.67) or lower than lower bound(-169645.08999999997). Cap them or remove them., has a high correlation with ['Total Volume', '4046', '4225']. Consider dropping one of them.</td>\n    </tr>\n    <tr>\n      <th id=\"T_ee81e_level0_row7\" class=\"row_heading level0 row7\" >Small Bags</th>\n      <td id=\"T_ee81e_row7_col0\" class=\"data row7 col0\" >float64</td>\n      <td id=\"T_ee81e_row7_col1\" class=\"data row7 col1\" >0.000000</td>\n      <td id=\"T_ee81e_row7_col2\" class=\"data row7 col2\" >NA</td>\n      <td id=\"T_ee81e_row7_col3\" class=\"data row7 col3\" >0.000000</td>\n      <td id=\"T_ee81e_row7_col4\" class=\"data row7 col4\" >8555283.680000</td>\n      <td id=\"T_ee81e_row7_col5\" class=\"data row7 col5\" >has 115 outliers greater than upper bound (216211.72) or lower than lower bound(-126383.96). Cap them or remove them., has a high correlation with ['Total Volume', '4046', '4225', '4770', 'Total Bags']. Consider dropping one of them.</td>\n    </tr>\n    <tr>\n      <th id=\"T_ee81e_level0_row8\" class=\"row_heading level0 row8\" >Large Bags</th>\n      <td id=\"T_ee81e_row8_col0\" class=\"data row8 col0\" >float64</td>\n      <td id=\"T_ee81e_row8_col1\" class=\"data row8 col1\" >0.000000</td>\n      <td id=\"T_ee81e_row8_col2\" class=\"data row8 col2\" >NA</td>\n      <td id=\"T_ee81e_row8_col3\" class=\"data row8 col3\" >0.000000</td>\n      <td id=\"T_ee81e_row8_col4\" class=\"data row8 col4\" >2214419.290000</td>\n      <td id=\"T_ee81e_row8_col5\" class=\"data row8 col5\" >has 133 outliers greater than upper bound (55577.945) or lower than lower bound(-33237.615). Cap them or remove them., has a high correlation with ['Total Bags']. Consider dropping one of them.</td>\n    </tr>\n    <tr>\n      <th id=\"T_ee81e_level0_row9\" class=\"row_heading level0 row9\" >XLarge Bags</th>\n      <td id=\"T_ee81e_row9_col0\" class=\"data row9 col0\" >float64</td>\n      <td id=\"T_ee81e_row9_col1\" class=\"data row9 col1\" >0.000000</td>\n      <td id=\"T_ee81e_row9_col2\" class=\"data row9 col2\" >NA</td>\n      <td id=\"T_ee81e_row9_col3\" class=\"data row9 col3\" >0.000000</td>\n      <td id=\"T_ee81e_row9_col4\" class=\"data row9 col4\" >93326.070000</td>\n      <td id=\"T_ee81e_row9_col5\" class=\"data row9 col5\" >has 182 outliers greater than upper bound (191.3) or lower than lower bound(-114.78). Cap them or remove them.</td>\n    </tr>\n    <tr>\n      <th id=\"T_ee81e_level0_row10\" class=\"row_heading level0 row10\" >type</th>\n      <td id=\"T_ee81e_row10_col0\" class=\"data row10 col0\" >object</td>\n      <td id=\"T_ee81e_row10_col1\" class=\"data row10 col1\" >0.000000</td>\n      <td id=\"T_ee81e_row10_col2\" class=\"data row10 col2\" >0</td>\n      <td id=\"T_ee81e_row10_col3\" class=\"data row10 col3\" >conventional</td>\n      <td id=\"T_ee81e_row10_col4\" class=\"data row10 col4\" >organic</td>\n      <td id=\"T_ee81e_row10_col5\" class=\"data row10 col5\" >No issue</td>\n    </tr>\n    <tr>\n      <th id=\"T_ee81e_level0_row11\" class=\"row_heading level0 row11\" >year</th>\n      <td id=\"T_ee81e_row11_col0\" class=\"data row11 col0\" >int64</td>\n      <td id=\"T_ee81e_row11_col1\" class=\"data row11 col1\" >0.000000</td>\n      <td id=\"T_ee81e_row11_col2\" class=\"data row11 col2\" >0</td>\n      <td id=\"T_ee81e_row11_col3\" class=\"data row11 col3\" >2015</td>\n      <td id=\"T_ee81e_row11_col4\" class=\"data row11 col4\" >2017</td>\n      <td id=\"T_ee81e_row11_col5\" class=\"data row11 col5\" >No issue</td>\n    </tr>\n    <tr>\n      <th id=\"T_ee81e_level0_row12\" class=\"row_heading level0 row12\" >region</th>\n      <td id=\"T_ee81e_row12_col0\" class=\"data row12 col0\" >object</td>\n      <td id=\"T_ee81e_row12_col1\" class=\"data row12 col1\" >0.000000</td>\n      <td id=\"T_ee81e_row12_col2\" class=\"data row12 col2\" >6</td>\n      <td id=\"T_ee81e_row12_col3\" class=\"data row12 col3\" >Albany</td>\n      <td id=\"T_ee81e_row12_col4\" class=\"data row12 col4\" >WestTexNewMexico</td>\n      <td id=\"T_ee81e_row12_col5\" class=\"data row12 col5\" >54 rare categories: Too many to list. Group them into a single category or drop the categories.</td>\n    </tr>\n  </tbody>\n</table>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas_dq\n",
    "import pandas as pd\n",
    "data = pd.read_csv('datasets\\data.csv')\n",
    "report = pandas_dq.dq_report(data, target=None, csv_engine=\"pandas\", verbose=1)\n",
    "reportJSON = report.to_json\n",
    "#print(reportJSON)\n",
    "reportDICT = report.to_dict()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alert: Detecting 199 duplicate rows...\n",
      "    Dropping 4046 which has a high correlation with ['Total Volume']\n",
      "    Dropping 4225 which has a high correlation with ['Total Volume', '4046']\n",
      "    Dropping 4770 which has a high correlation with ['Total Volume', '4046', '4225']\n",
      "    Dropping Total Bags which has a high correlation with ['Total Volume', '4046', '4225']\n",
      "    Dropping Small Bags which has a high correlation with ['Total Volume', '4046', '4225', '4770', 'Total Bags']\n",
      "    Dropping Large Bags which has a high correlation with ['Total Bags']\n",
      "Alert: Dropping 199 duplicate rows can sometimes cause column data types to change to object. Double-check!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:878: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "from pandas_dq import Fix_DQ\n",
    "fdq = Fix_DQ()\n",
    "\n",
    "X_train_transformed = fdq.fit_transform(data)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alert: Dropping 16 duplicate rows can sometimes cause column data types to change to object. Double-check!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robin\\AppData\\Local\\Temp\\ipykernel_19504\\3440526732.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  report = pandas_dq.dq_report(X_train_transformed, target=None, csv_engine=\"pandas\", verbose=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<pandas.io.formats.style.Styler at 0x216d58e5870>",
      "text/html": "<style type=\"text/css\">\n#T_7e219_row0_col0, #T_7e219_row0_col2, #T_7e219_row0_col3, #T_7e219_row0_col4, #T_7e219_row0_col5, #T_7e219_row1_col0, #T_7e219_row1_col2, #T_7e219_row1_col3, #T_7e219_row1_col4, #T_7e219_row1_col5, #T_7e219_row2_col0, #T_7e219_row2_col2, #T_7e219_row2_col3, #T_7e219_row2_col4, #T_7e219_row2_col5, #T_7e219_row3_col0, #T_7e219_row3_col2, #T_7e219_row3_col3, #T_7e219_row3_col4, #T_7e219_row3_col5, #T_7e219_row4_col0, #T_7e219_row4_col2, #T_7e219_row4_col3, #T_7e219_row4_col4, #T_7e219_row4_col5, #T_7e219_row5_col0, #T_7e219_row5_col2, #T_7e219_row5_col3, #T_7e219_row5_col4, #T_7e219_row5_col5, #T_7e219_row6_col0, #T_7e219_row6_col2, #T_7e219_row6_col3, #T_7e219_row6_col4, #T_7e219_row6_col5 {\n  font-family: Segoe UI;\n}\n#T_7e219_row0_col1, #T_7e219_row1_col1, #T_7e219_row2_col1, #T_7e219_row3_col1, #T_7e219_row4_col1, #T_7e219_row5_col1, #T_7e219_row6_col1 {\n  background-color: #fff5f0;\n  color: #000000;\n  font-family: Segoe UI;\n}\n</style>\n<table id=\"T_7e219\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_7e219_level0_col0\" class=\"col_heading level0 col0\" >Data Type</th>\n      <th id=\"T_7e219_level0_col1\" class=\"col_heading level0 col1\" >Missing Values%</th>\n      <th id=\"T_7e219_level0_col2\" class=\"col_heading level0 col2\" >Unique Values%</th>\n      <th id=\"T_7e219_level0_col3\" class=\"col_heading level0 col3\" >Minimum Value</th>\n      <th id=\"T_7e219_level0_col4\" class=\"col_heading level0 col4\" >Maximum Value</th>\n      <th id=\"T_7e219_level0_col5\" class=\"col_heading level0 col5\" >DQ Issue</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_7e219_level0_row0\" class=\"row_heading level0 row0\" >Date</th>\n      <td id=\"T_7e219_row0_col0\" class=\"data row0 col0\" >object</td>\n      <td id=\"T_7e219_row0_col1\" class=\"data row0 col1\" >0.000000</td>\n      <td id=\"T_7e219_row0_col2\" class=\"data row0 col2\" >0</td>\n      <td id=\"T_7e219_row0_col3\" class=\"data row0 col3\" >Rare</td>\n      <td id=\"T_7e219_row0_col4\" class=\"data row0 col4\" >Rare</td>\n      <td id=\"T_7e219_row0_col5\" class=\"data row0 col5\" >Zero-variance colum: drop before modeling process.</td>\n    </tr>\n    <tr>\n      <th id=\"T_7e219_level0_row1\" class=\"row_heading level0 row1\" >AveragePrice</th>\n      <td id=\"T_7e219_row1_col0\" class=\"data row1 col0\" >float64</td>\n      <td id=\"T_7e219_row1_col1\" class=\"data row1 col1\" >0.000000</td>\n      <td id=\"T_7e219_row1_col2\" class=\"data row1 col2\" >NA</td>\n      <td id=\"T_7e219_row1_col3\" class=\"data row1 col3\" >0.460000</td>\n      <td id=\"T_7e219_row1_col4\" class=\"data row1 col4\" >2.750000</td>\n      <td id=\"T_7e219_row1_col5\" class=\"data row1 col5\" >has 7 outliers greater than upper bound (2.51) or lower than lower bound(0.19000000000000028). Cap them or remove them.</td>\n    </tr>\n    <tr>\n      <th id=\"T_7e219_level0_row2\" class=\"row_heading level0 row2\" >Total Volume</th>\n      <td id=\"T_7e219_row2_col0\" class=\"data row2 col0\" >float64</td>\n      <td id=\"T_7e219_row2_col1\" class=\"data row2 col1\" >0.000000</td>\n      <td id=\"T_7e219_row2_col2\" class=\"data row2 col2\" >NA</td>\n      <td id=\"T_7e219_row2_col3\" class=\"data row2 col3\" >5.957261</td>\n      <td id=\"T_7e219_row2_col4\" class=\"data row2 col4\" >13.930087</td>\n      <td id=\"T_7e219_row2_col5\" class=\"data row2 col5\" >No issue</td>\n    </tr>\n    <tr>\n      <th id=\"T_7e219_level0_row3\" class=\"row_heading level0 row3\" >XLarge Bags</th>\n      <td id=\"T_7e219_row3_col0\" class=\"data row3 col0\" >float64</td>\n      <td id=\"T_7e219_row3_col1\" class=\"data row3 col1\" >0.000000</td>\n      <td id=\"T_7e219_row3_col2\" class=\"data row3 col2\" >NA</td>\n      <td id=\"T_7e219_row3_col3\" class=\"data row3 col3\" >-0.637999</td>\n      <td id=\"T_7e219_row3_col4\" class=\"data row3 col4\" >1.425258</td>\n      <td id=\"T_7e219_row3_col5\" class=\"data row3 col5\" >No issue</td>\n    </tr>\n    <tr>\n      <th id=\"T_7e219_level0_row4\" class=\"row_heading level0 row4\" >type</th>\n      <td id=\"T_7e219_row4_col0\" class=\"data row4 col0\" >object</td>\n      <td id=\"T_7e219_row4_col1\" class=\"data row4 col1\" >0.000000</td>\n      <td id=\"T_7e219_row4_col2\" class=\"data row4 col2\" >0</td>\n      <td id=\"T_7e219_row4_col3\" class=\"data row4 col3\" >conventional</td>\n      <td id=\"T_7e219_row4_col4\" class=\"data row4 col4\" >organic</td>\n      <td id=\"T_7e219_row4_col5\" class=\"data row4 col5\" >No issue</td>\n    </tr>\n    <tr>\n      <th id=\"T_7e219_level0_row5\" class=\"row_heading level0 row5\" >year</th>\n      <td id=\"T_7e219_row5_col0\" class=\"data row5 col0\" >int64</td>\n      <td id=\"T_7e219_row5_col1\" class=\"data row5 col1\" >0.000000</td>\n      <td id=\"T_7e219_row5_col2\" class=\"data row5 col2\" >0</td>\n      <td id=\"T_7e219_row5_col3\" class=\"data row5 col3\" >2015</td>\n      <td id=\"T_7e219_row5_col4\" class=\"data row5 col4\" >2017</td>\n      <td id=\"T_7e219_row5_col5\" class=\"data row5 col5\" >No issue</td>\n    </tr>\n    <tr>\n      <th id=\"T_7e219_level0_row6\" class=\"row_heading level0 row6\" >region</th>\n      <td id=\"T_7e219_row6_col0\" class=\"data row6 col0\" >object</td>\n      <td id=\"T_7e219_row6_col1\" class=\"data row6 col1\" >0.000000</td>\n      <td id=\"T_7e219_row6_col2\" class=\"data row6 col2\" >0</td>\n      <td id=\"T_7e219_row6_col3\" class=\"data row6 col3\" >Rare</td>\n      <td id=\"T_7e219_row6_col4\" class=\"data row6 col4\" >Rare</td>\n      <td id=\"T_7e219_row6_col5\" class=\"data row6 col5\" >Zero-variance colum: drop before modeling process.</td>\n    </tr>\n  </tbody>\n</table>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "report = pandas_dq.dq_report(X_train_transformed, target=None, csv_engine=\"pandas\", verbose=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import cleanlab\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "df = pd.read_csv('datasets\\Iris.csv')\n",
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "# Fit and transform the \"Species\" column\n",
    "df['Species'] = le.fit_transform(df['Species'])\n",
    "def cleanlab_test(df):\n",
    "\n",
    "    # cleanlab works with **any classifier**. Yup, you can use PyTorch/TensorFlow/OpenAI/XGBoost/etc.\n",
    "    cl = cleanlab.classification.CleanLearning(RandomForestClassifier())\n",
    "    data = df.loc[:, df.columns != 'class']\n",
    "\n",
    "    # cleanlab finds data and label issues in **any dataset**... in ONE line of code!\n",
    "    # Fit model to messy, real-world data, automatically training on cleaned data.\n",
    "    _ = cl.fit(data, df['class'])\n",
    "    # See the label quality for every example, which data has issues, and more.\n",
    "    print(cl.get_label_issues())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "from deepchecks.tabular import Dataset\n",
    "import deepchecks.tabular.checks\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from deepchecks.tabular.datasets.classification.phishing import load_data\n",
    "import numpy as np\n",
    "from deepchecks.tabular.datasets.classification import adult\n",
    "import plotly.express as px\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "def encode_categorical_columns(dataset, target, data_types):\n",
    "    # Find all categorical columns\n",
    "    # TODO: regel hieronder aanpassen naar wat de user heeft ingegeven (let hierbij op strings als dtype, moeten gezien worden als categorical)\n",
    "    categorical_cols = dataset.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    target_is_categorical = False\n",
    "    if target != 'None':\n",
    "        # remove target as we want to label encode this (for classification problems)\n",
    "        if target in categorical_cols:\n",
    "            target_is_categorical = True\n",
    "            categorical_cols.remove(target)\n",
    "\n",
    "            # label encode target\n",
    "            le = LabelEncoder()\n",
    "            encoded_target = le.fit_transform(dataset[target])\n",
    "            # replace target column with label encoded values\n",
    "            dataset.drop(columns=[target], inplace=True)\n",
    "            dataset[target] = encoded_target\n",
    "\n",
    "    if not categorical_cols:  # then no features are categorical, and we're done\n",
    "        return dataset\n",
    "\n",
    "    # if there are categorical columns, we want to one-hot-encode them\n",
    "\n",
    "    # encode categoricals\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', max_categories=100)\n",
    "    encoded_columns = encoder.fit_transform(dataset[categorical_cols])\n",
    "    new_columns = pd.DataFrame(encoded_columns.toarray(), columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "    # add new columns to df and drop old ones\n",
    "    dataset_encoded = pd.concat([dataset, new_columns], axis=1)\n",
    "    dataset_encoded = dataset_encoded.drop(columns=categorical_cols)\n",
    "\n",
    "    # reposition target column to the end of the dataframe\n",
    "    if target != 'None' and target_is_categorical:\n",
    "        dataset_encoded.drop(columns=[target], inplace=True)\n",
    "        dataset_encoded[target] = encoded_target\n",
    "\n",
    "    #XGBClassifier doesn't accept: [, ] or <, so loop over the columns and change the names if they contain such values\n",
    "    new_col_names = {col: col.replace('<', '(smaller than)').replace('[', '(').replace(']', ')') for col in dataset_encoded.columns}\n",
    "    dataset_encoded = dataset_encoded.rename(columns=new_col_names)\n",
    "\n",
    "    return dataset_encoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "def wrong_label(encoded_dataset, target):\n",
    "    \"\"\"\"Function that finds potential label errors (due to annotator mistakes), edge cases, and otherwise ambiguous examples\"\"\"\n",
    "    model_XGBC = XGBClassifier(tree_method=\"hist\", enable_categorical=True) #hist is fastest tree method of XGBoost\n",
    "    #TODO: add more models and choose whichever gives highest prediction? Downside --> slows process\n",
    "    data_no_labels = encoded_dataset.drop(columns=[target])\n",
    "    labels = encoded_dataset[target]\n",
    "    pred_probs = cross_val_predict(model_XGBC, data_no_labels, labels, method='predict_proba')\n",
    "\n",
    "    preds = np.argmax(pred_probs, axis=1)\n",
    "    acc_original = accuracy_score(preds, labels)\n",
    "    print(f\"Accuracy with original data: {round(acc_original * 100, 1)}%\")\n",
    "\n",
    "    #use cleanlab's built in confident learning method to find label issues\n",
    "    cl = cleanlab.classification.CleanLearning()\n",
    "    issues_dataframe = cl.find_label_issues(X=None, labels=labels, pred_probs=pred_probs)\n",
    "    wrong_label_count = (issues_dataframe['is_label_issue'] == True).sum()\n",
    "\n",
    "    #filter df so only errors are visible\n",
    "    issues_dataframe_only_errors = issues_dataframe[issues_dataframe['is_label_issue'] == True]\n",
    "\n",
    "    return issues_dataframe, issues_dataframe_only_errors, wrong_label_count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with original data: 75.7%\n",
      "   is_label_issue  label_quality  given_label  predicted_label\n",
      "0           False       0.982974            1                1\n",
      "1           False       0.896697            0                0\n",
      "2           False       0.995315            1                1\n",
      "3            True       0.405909            1                0\n",
      "4           False       0.830484            0                0\n"
     ]
    },
    {
     "data": {
      "text/plain": "(     is_label_issue  label_quality  given_label  predicted_label\n 0             False       0.982974            1                1\n 1             False       0.896697            0                0\n 2             False       0.995315            1                1\n 3              True       0.405909            1                0\n 4             False       0.830484            0                0\n ..              ...            ...          ...              ...\n 995           False       0.995562            1                1\n 996           False       0.788997            1                1\n 997           False       0.996626            1                1\n 998           False       0.964832            0                0\n 999           False       0.864109            1                1\n \n [1000 rows x 4 columns],\n 200)"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.io.arff import loadarff\n",
    "\n",
    "raw_data = loadarff('datasets/dataset_31_credit-g.arff')\n",
    "df_data = pd.DataFrame(raw_data[0])\n",
    "#df_data.drop(columns=['checking_status', 'savings_status', 'employment'], inplace=True)\n",
    "encdoded_data = encode_categorical_columns(df_data, 'class', None)\n",
    "# pred_probs, labels = wrong_label(encdoded_data, 'class')\n",
    "\n",
    "wrong_label(encdoded_data, 'class')\n",
    "\n",
    "# cl = cleanlab.classification.CleanLearning()\n",
    "# issues_dataframe = cl.find_label_issues(X=None, labels=labels, pred_probs=pred_probs, return_indices_ranked_by='self_confidence')\n",
    "# print(issues_dataframe)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
